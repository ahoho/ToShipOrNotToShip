{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wilcoxon, kendalltau\n",
    "\n",
    "from evaluation.tools import load_data, get_equal_pairs, get_valid\n",
    "from evaluation.SETTINGS import investigated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated data loaded\n"
     ]
    }
   ],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics cannot be aggregated via mean (e.g., BLEU). Want to cut these out of the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahoyl\\AppData\\Local\\Temp/ipykernel_1956/489513427.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  mean_pct_diffs[metric] = (sentence_mean - corpus_val) / corpus_val\n"
     ]
    }
   ],
   "source": [
    "mean_to_corpus_diffs, mean_to_corpus_pct_diffs = [], []\n",
    "\n",
    "for campaign in data:\n",
    "    for system in data[campaign]:\n",
    "        human_data = data[campaign][system]['hum_annotations']\n",
    "        auto_data = data[campaign][system]['hum_only_automatic_metrics']\n",
    "        if isinstance(auto_data, dict) and auto_data == {}:\n",
    "            auto_data = data[campaign][system]['automatic_metrics']\n",
    "        if isinstance(auto_data, pd.DataFrame):\n",
    "            auto_data = auto_data.set_index('Unnamed: 0').to_dict()[0]\n",
    "        human_data_means = human_data[[col for col in human_data if col.startswith(\"metric\")]].mean()\n",
    "        mean_diffs, mean_pct_diffs = {}, {}\n",
    "        for metric, corpus_val in auto_data.items():\n",
    "            try:\n",
    "                sentence_mean = human_data_means[f\"metric_{metric}\"]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "            mean_diffs[metric] = (sentence_mean - corpus_val)\n",
    "            mean_pct_diffs[metric] = (sentence_mean - corpus_val) / corpus_val\n",
    "        mean_to_corpus_diffs.append(mean_diffs)\n",
    "        mean_to_corpus_pct_diffs.append(mean_pct_diffs)\n",
    "\n",
    "mean_to_corpus_diffs = pd.DataFrame(mean_to_corpus_diffs)\n",
    "mean_to_corpus_pct_diffs = pd.DataFrame(mean_to_corpus_pct_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pct_diffs = mean_to_corpus_pct_diffs.abs().max()\n",
    "investigated_metrics = list(max_pct_diffs.loc[np.isclose(max_pct_diffs, 0, atol=1e-5)].index)\n",
    "investigated_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mean_to_corpus_pct_diffs * 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a large dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = []\n",
    "for campaign in data:\n",
    "    for system in data[campaign]:\n",
    "        human_data = data[campaign][system]['hum_annotations']\n",
    "        human_data[\"campaign\"] = campaign\n",
    "        human_data[\"system\"] = system\n",
    "        data_df.append(human_data)\n",
    "data_df = pd.concat(data_df, ignore_index=True)\n",
    "data_df = data_df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 82/1728 [00:13<03:54,  7.01it/s]C:\\Users\\ahoyl\\.conda\\envs\\sim-compare\\lib\\site-packages\\scipy\\stats\\morestats.py:3155: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "  6%|▌         | 96/1728 [00:15<04:26,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System appears to have been repeated, dropping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 242/1728 [00:38<04:10,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System appears to have been repeated, dropping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 749/1728 [02:07<02:28,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System appears to have been repeated, dropping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1728/1728 [04:53<00:00,  5.89it/s]\n"
     ]
    }
   ],
   "source": [
    "METRIC = \"Prism_ref\" # just a test\n",
    "\n",
    "# data_df = get_valid(data_df)\n",
    "\n",
    "results = []\n",
    "for campaign in tqdm(data_df.campaign.unique()):\n",
    "    campaign_df = data_df.loc[data_df.campaign == campaign]\n",
    "    systems = campaign_df.system.unique()\n",
    "    for system_a, system_b in combinations(systems, 2):\n",
    "        system_a_df = campaign_df.loc[campaign_df.system == system_a]\n",
    "        system_b_df = campaign_df.loc[campaign_df.system == system_b]\n",
    "        \n",
    "        # below pulled from the original analysis.py \n",
    "        skip_pair = False\n",
    "        for metric in investigated_metrics:\n",
    "            if metric not in data[campaign][system_a]['automatic_metrics'] or metric not in data[campaign][system_b]['automatic_metrics']:\n",
    "                skip_pair = True\n",
    "        if skip_pair:\n",
    "            continue\n",
    "\n",
    "        # get automated score diff\n",
    "        for metric in investigated_metrics:\n",
    "            auto_score_diff = system_a_df[f\"metric_{METRIC}\"].mean() - system_b_df[f\"metric_{METRIC}\"].mean()\n",
    "\n",
    "        # get human score diff\n",
    "        system_a_df, system_b_df = get_valid(system_a_df), get_valid(system_b_df)\n",
    "        if len(system_a_df) != len(system_b_df) or sum(abs(system_a_df['SegmentID'] - system_b_df['SegmentID'])) != 0:\n",
    "            system_a_df, system_b_df = get_equal_pairs(system_a_df, system_b_df)\n",
    "            \n",
    "            # double check that segment ids are equal\n",
    "            if sum(abs(system_a_df['SegmentID']-system_b_df['SegmentID'])) != 0:\n",
    "                raise ValueError(\"SegmentIDs are not equal\")\n",
    "\n",
    "        human_score_diff = system_a_df[\"Score\"].mean() - system_b_df[\"Score\"].mean()\n",
    "\n",
    "        differences = (system_a_df[\"Score\"] - system_b_df[\"Score\"]).to_list()\n",
    "        if np.sum(differences) == 0:\n",
    "            # print(\"System appears to have been repeated, dropping\")\n",
    "            continue \n",
    "\n",
    "        if len(differences) < 100:\n",
    "            raise ValueError(\"There is too few lines, something is wrong!\")\n",
    "\n",
    "        t_stat, p_value_twosided = wilcoxon(differences, alternative='two-sided')\n",
    "\n",
    "        result = {\n",
    "            \"campaign\": campaign,\n",
    "            \"system_a\": system_a,\n",
    "            \"system_b\": system_b,\n",
    "            \"human_score_diff\": human_score_diff,\n",
    "            \"human_p_value\": p_value_twosided,\n",
    "            \"auto_score_diff\": auto_score_diff,\n",
    "            \"accuracy\": np.sign(auto_score_diff) == np.sign(human_score_diff)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 3344, 1.000, 0.806\n",
      "N: 1717, 0.050, 0.945\n",
      "N: 1420, 0.010, 0.970\n",
      "N: 1176, 0.001, 0.983\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1, 0.05, 0.01, 0.001]:\n",
    "    acc = results.loc[results.human_p_value <= alpha, \"accuracy\"]\n",
    "    print(f\"N: {len(acc)}, {alpha:0.03f}, {acc.mean():0.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./statistical_data.pickle\", \"rb\") as infile:\n",
    "    statistical_data = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_original = pd.DataFrame(statistical_data)\n",
    "\n",
    "results_original['accuracy'] = np.sign(results_original['human1']) == np.sign(results_original[METRIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    results.replace(\"\\\\.xlsx\", \"\", regex=True)\n",
    "           .rename({\n",
    "               \"system_a\": \"SystemAid\",\n",
    "               \"system_b\": \"SystemBid\",\n",
    "               \"human_p_value\": \"human_p_value_new\",\n",
    "               \"accuracy\": \"accuracy_new\"\n",
    "            },\n",
    "            axis='columns',\n",
    "            )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merged = results_original[['campaign', 'SystemAid', 'SystemBid', 'accuracy', 'human_pvalue', METRIC]].merge(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merged['diff_of_diff'] = np.abs(results_merged[METRIC] - results_merged['auto_score_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "campaign             ffdcaad19e1e58a70563371fad02c141\n",
       "SystemAid            f7e17531e5935e6d6be48f6f3fba5b00\n",
       "SystemBid            ffd2d560c712ac3cb62dbca7675790f5\n",
       "accuracy                                         True\n",
       "human_pvalue                                      1.0\n",
       "Prism_ref                                    1.099608\n",
       "human_score_diff                            28.482105\n",
       "human_p_value_new                                 1.0\n",
       "auto_score_diff                              1.099608\n",
       "accuracy_new                                     True\n",
       "diff_of_diff                                      0.0\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_merged.sort_values('diff_of_diff').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004476666706314303"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "campaign_df = data_df.loc[data_df.campaign == 'd38a89488e5ed4e0706a849a640a99a0']\n",
    "system_a_df = campaign_df.loc[campaign_df.system == '13f1d5f1838280fa11793328b00eace6.xlsx']\n",
    "system_b_df = campaign_df.loc[campaign_df.system == 'b91985dbc06ea031b20b09a24d6c3d43.xlsx']\n",
    "\n",
    "system_a_df.metric_Prism_ref.mean() - system_b_df.metric_Prism_ref.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "* Probably do all analyses with a single language pair, to start.\n",
    "* Randomly subsample data at multiple sizes (1%, 5%, etc.). For each size, show variance across each \"seed\" pool (e.g., the initial set of campaigns)\n",
    "    * Subsample by campaign, but otherwise maintain same strategy (basically what happened above with the reduced release). Equivalent to running fewer system pairs.\n",
    "    * Subsample by campaign, but pool data and partition to create pseudo-systems.\n",
    "    * Subsample by language pair: pooled dataset is partitioned to create pseudo-systems.\n",
    "    * Subsample completely at random, across languages.\n",
    "* Strategically subsample: pick campaigns such that spread is maximized/minimized across, say, geom mean of the different metric types\n",
    "    * Strategically partition (?) by weighing (?) examples s.t. differences between pseudo-systems show large variation (not all \"close\" to one another)\n",
    "* Redo the above for the 'precision' (since this table is basically 'recall'): when a metric tells you \"difference\", how often is there a sig. human difference?\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67727b7f3d95b6632138c7ba5fded2c6b4b3f31c72d9288f080bf4c80fd4c34e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
